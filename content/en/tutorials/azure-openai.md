---
title: Azure - OpenAI
description: Information about running Azure OpenAI
translationKey: tutorials/azure-openai
draft: false
tags:
  - AWS
  - IAM
  - CLI
categories:
  - Tutorials
---

{{< notice title="Note">}}
We have a bit of a saying, "Welcome to the Cloud! A new experience every day!".  
Unfortunately this is particularly relevant to AI/ML, which seems to change even more rapidly. Please forgive us (and let us know) for anything that is out of date here!
{{< /notice >}}

## Pricing

Most Azure OpenAI pricing is by tokens, which of course are an AI/ML way of representing (usually) text. As a rule of thumb, one token tends to be equal to about 4.5 characters; so short, common words are usually one token, with longer or more obscure jargon being more. There are tools out there like [TikToken](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) that can measure content without having to send it to OpenAI, if that's something you need.

### Actual Costs

[Azure OpenAI pricing](https://azure.microsoft.com/en-ca/pricing/details/cognitive-services/openai-service/) is the authoritative resource.

- Don't forget to set your region & currency!
- Make sure you're looking at *exactly* the right model; `o4`, `4o`, and `4o-mini` are all very different!
- Note the different type of tokens:
  - "Input" are tokens sent to the model
  - "Cached" are tokens that have already been sent to the model in a previous turn (i.e. followup questions). For models that don't support caching, *all* tokens, including those previously prompted, are counted as input, and caching is only available on certain models.
  - "Output" are the tokens generated by the model. As a rule of thumb, cost 4Ã— more than Input tokens.
- Batch requires a different API/app logic, but costs about 50% vs realtime.

## Model Availability

See [Deployment types](#deployment-types) below for a deeper dive, but our straightforward reccomendations depend on your experiment type:

## Profile 1 (Unclassified)

If you're running an unclassified experiment (i.e., *not* Protected B) experiment, you can actually use models from any region. The easiest deployment types are either **Global Standard** or **Global Batch**; if you want the best selection of models, the two regions of choice are:

- East US 2
- Sweden Central

See here for the region table for [Global Standard model availability](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai&tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard#global-standard-model-availability).

## Profile 3 (Protected B)

Protected B data **must** be stored in Canada, although it *can* be processed outside of Canada (so long as it remains encrypted in transit).

{{< notice title="Note">}}
Although TBS policy does not *require* Protected B to be processed domestically, this does not mean your project's ATO/SA&A (Authority to operate; Security Audit & Authorization) will accept this risk. As a result, you may want to plan for the very decent chance you'll be required to keep everything, including processing, in Canada.

See here for a [backgrounder on ATO/SA&As](https://www.canada.ca/en/shared-services/corporate/publications/audit-security-assessment-authorization-march-2020.html).
{{< /notice >}}

In practice, with Azure OpenAI, this means you need to deploy your resources in Canada (specifically **Canada East** for the best options).
In our sandbox space, we do permit processing outside of Canada by using Global deployments.

Please still use your judgement, if processing your Protected B data outside of Canada makes sense!

If you want to process in Canada, [Standard deployment model availability](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai&tabs=standard%2Cstandard-chat-completions%2Cglobal-standard#standard-deployment-model-availability) lists the options (as of October 2025):

- gpt-4o, version 2024-11-20
- gpt-35-turbo, version 1106 or 0125

If you decide to process outside of Canada, you can use models per [Global Standard model availability](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai&tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard#global-standard-model-availability).

## Moving to Production

In our experience, almost all production cloud spaces are setup to support Protected B, meaning even if your project does not use Protected B data, you should still expect the restrictions of Protected B (see [Protected B experiments](#profile-3-protected-b) above).
Note that in production you may also have the option of *Provisioned* deployments, which are essentially where you make a commitment of tokens/hour, but there are a couple more models offered.

## Deployment Types

In Azure OpenAI, there is primarily two 'dimensions' to how you deploy models"

### Geographic

- **Standard** or **Regional**  
  Models are processed in the region of the (OpenAI or AI Foundry) resource they're attached to.
- **Global**  
  Ss the name implies, more or less ignores the region of your resource and will process your request in whatever Azure datacenter that can serve you best. Strangely however, you can't deploy *all* models as Global in all regions; see the [Unclassified experiments](#profile-1-unclassified) section above for our reccomendation.
- **Data Zone**  
  A special one that doesn't really apply to us; it's like Global but limits processing to the same geopolitical boundary the resource is deployed in: either the US or EU.

### Type

- **Standard**  
  The baseline option, and should be used for most deployments.
- **Provisioned** or **Provisioned Managed**  
  More or less like a commitment tier, where you commit to some hourly number of tokens (using an absurdly complicated "PTU" metric). GPT-4o in this deployment costs CAD$277/h for 50M tokens.  
  Since this is pretty unlikely to be appropriately used in our environments, we've used Azure Policy to prevent accidental deployment of this type; they can be safely ignored.
- **Batch**  
  For batched requests, where responses aren't required immediately. Although you do need to adjust your API calls / application logic to support it, batch can save a lot of cost, as Microsoft seems to price it at 50% of the equivalent non-batch price.
  
The above Geographic & Type options are combined to form the ~8 different possible ways to deploy a model; unfortunately the use of the word 'Standard' could imply *either* the geographic or type, and is sometimes entirely omitted.
